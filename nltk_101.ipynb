{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK 101\n",
    "\n",
    "- Tokenization\n",
    "- Frequency Distributions\n",
    "- Corpus - Isolated, Categorical, Overlapping, Temporal etc.\n",
    "- Web Scraping using BeautifulSoup\n",
    "- Regular Expressions for Tokenization\n",
    "- Bigrams and Collocations\n",
    "- Stemming\n",
    "- Lemmatization\n",
    "- POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Python is an interpreted high-level programming language for general-purpose programming. Created by Guido van Rossum and first released in 1991.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In NLTK, **sent_tokenize** function generates **sentences** from the given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(text)\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In NLTK, **word_tokenize** function generates **words** from the given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = nltk.word_tokenize(text)\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Python', 'is', 'an', 'interpreted', 'high-level']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Frequency Distribution of Words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordfreq = nltk.FreqDist(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('programming', 2), ('.', 2)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordfreq.most_common(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "triangular; triangular; triangular; triangular\n"
     ]
    }
   ],
   "source": [
    "text1.findall(\"<tri.*r>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.text.Text"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "260819"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_words = len(text1)\n",
    "n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19317"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_unique_words = len(set(text1))\n",
    "n_unique_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting lower case unique words from text1 example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19317"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1_lcw = [word.lower() for word in set(text1)]\n",
    "len(text1_lcw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17231"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_unique_words_lc = len(set(text1_lcw))\n",
    "n_unique_words_lc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.502044830977896"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_coverage1 = n_words / n_unique_words\n",
    "word_coverage1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.136614241773549"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_coverage2 = n_words / n_unique_words_lc\n",
    "word_coverage2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['uninterpenetratingly']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_words = [word for word in set(text1) if len(word) > 18]\n",
    "big_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sunset', 'Sunda', 'Sunday']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sun_words = [word for word in set(text1) if word.startswith('Sun')]\n",
    "sun_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 18713), ('the', 13721), ('.', 6862)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1_freq = nltk.FreqDist(text1)\n",
    "text1_freq.most_common(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1_freq['Sunday']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_uncommon_words = [word for word in text1 if word.isalpha() and len(word) > 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Queequeg', 252), ('Starbuck', 196), ('something', 119)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1_uncommon_freq = nltk.FreqDist(large_uncommon_words)\n",
    "text1_uncommon_freq.most_common(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import genesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**genesis** is an **isolated** corpus - holds individual text collections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['english-kjv.txt',\n",
       " 'english-web.txt',\n",
       " 'finnish.txt',\n",
       " 'french.txt',\n",
       " 'german.txt',\n",
       " 'lolcat.txt',\n",
       " 'portuguese.txt',\n",
       " 'swedish.txt']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genesis.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 30 english-kjv.txt\n",
      "4 19 english-web.txt\n",
      "5 15 finnish.txt\n",
      "4 23 french.txt\n",
      "4 23 german.txt\n",
      "4 20 lolcat.txt\n",
      "4 27 portuguese.txt\n",
      "4 30 swedish.txt\n"
     ]
    }
   ],
   "source": [
    "for fileid in genesis.fileids():\n",
    "    # Raw: Characters\n",
    "    n_chars = len(genesis.raw(fileid))\n",
    "    n_words = len(genesis.words(fileid))\n",
    "    n_sents = len(genesis.sents(fileid))\n",
    "    print(int(n_chars/n_words), int(n_words/n_sents), fileid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.corpus import PlaintextCorpusReader\n",
    "# # Path to text files on drive\n",
    "# wordlists = PlaintextCorpusReader(., '.*')\n",
    "# wordlists.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'apple': 2, 'cabbage': 2, 'kiwi': 1, 'potato': 1})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items = ['apple', 'apple', 'kiwi', 'cabbage', 'cabbage', 'potato']\n",
    "nltk.FreqDist(items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conditional Frequency Distribution** takes in the list of tuples in below example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F', 'V']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_items = [('F','apple'), ('F','apple'), ('F','kiwi'), ('V','cabbage'), ('V','cabbage'), ('V','potato')]\n",
    "cfd = nltk.ConditionalFreqDist(c_items)\n",
    "cfd.conditions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'cabbage': 2, 'potato': 1})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfd['V']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **brown** is a **categorical** corpus \n",
    "- **reuters** is an **overlapping** corpus - each text collection tagged to one or more categories\n",
    "- **inaugural** is a **temporal** corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ConditionalFreqDist with 15 conditions>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "cfd_brown = nltk.ConditionalFreqDist([(genre, word) for genre in brown.categories() for word in brown.words(categories=genre)])\n",
    "print(cfd_brown)\n",
    "cfd_brown.conditions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           leadership    worship   hardship \n",
      "government         12          3          2 \n",
      "     humor          1          0          0 \n",
      "   reviews         14          1          2 \n"
     ]
    }
   ],
   "source": [
    "cfd_brown.tabulate(conditions=['government', 'humor', 'reviews'], samples=['leadership', 'worship', 'hardship'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           leadership    worship   hardship \n",
      "government         12         15         17 \n",
      "     humor          1          1          1 \n",
      "   reviews         14         15         17 \n"
     ]
    }
   ],
   "source": [
    "cfd_brown.tabulate(conditions=['government', 'humor', 'reviews'], samples=['leadership', 'worship', 'hardship'], cumulative = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 5580), (',', 5188), ('.', 4030)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_fd = cfd_brown['news']\n",
    "news_fd.most_common(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5580"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_fd['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(fid, name): [('female.txt', 'Abagael'), ('female.txt', 'Abagail'), ('female.txt', 'Abbe'), ('female.txt', 'Abbey'), ('female.txt', 'Abbi')]\n",
      "(fid, last char in name): [('female', 'l'), ('female', 'l'), ('female', 'e'), ('female', 'y'), ('female', 'i')]\n",
      "['female', 'male']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import names\n",
    "nt = [(fid, name) for fid in names.fileids() for name in names.words(fid)]\n",
    "print(\"(fid, name): {}\".format(nt[:5]))\n",
    "nt = [(fid.split('.')[0], name[-1]) for fid in names.fileids() for name in names.words(fid)]\n",
    "print(\"(fid, last char in name): {}\".format(nt[:5]))\n",
    "cfd_names = nltk.ConditionalFreqDist(nt)\n",
    "print(cfd_names.conditions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfd_names['female'] > cfd_names['male']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          a    e \n",
      "female 1773 1432 \n",
      "  male   29  468 \n"
     ]
    }
   ],
   "source": [
    "cfd_names.tabulate(samples=['a', 'e'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
    "content1 = request.urlopen(url).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://www.bbc.com/news/health-42802191\"\n",
    "html_content = request.urlopen(url).read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beautifulsoup\n",
    "- Module used for scraping the required text from the webpages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_body = soup.find_all('div', attrs={'class':'story-body__inner'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_text = [elm.text for elm in inner_body[0].find_all(['h1', 'h2', 'p', 'li'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_content2 = '\\n'.join(inner_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third party libraries such as ****pywin32, pypdf**** are required for accessing binary data as in Microsoft Word or PDF documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Project', 'Gutenberg', 'EBook', 'of', 'Crime']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_content1 = content1.decode('unicode_escape')  # Converts bytes to unicode\n",
    "tokens1 = nltk.word_tokenize(text_content1)\n",
    "tokens1[3:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "751\n",
      "['Smokers', 'need', 'to', 'quit', 'cigarettes']\n"
     ]
    }
   ],
   "source": [
    "tokens2 = nltk.word_tokenize(text_content2)\n",
    "print(len(tokens2))\n",
    "print(tokens2[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Expressions for Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "668\n",
      "['Smokers', 'need', 'to', 'quit', 'cigarettes']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "tokens2_2 = re.findall(r'\\w+', text_content2)\n",
    "print(len(tokens2_2))\n",
    "print(tokens2_2[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "668"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alternatively use regexp_tokenize function from NLTK\n",
    "pattern = r'\\w+'\n",
    "tokens2_3 = nltk.regexp_tokenize(text_content2, pattern)\n",
    "len(tokens2_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.text.Text"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text2 = nltk.Text(tokens2)\n",
    "type(input_text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With ****word_tokenize****, punctuation characters are treated as words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Python', 'is', 'cool', '!', '!', '!']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize('Python is cool!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With ****regexp_tokenize****, punctuation characters are not treated as words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Python', 'is', 'cool']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.regexp_tokenize('Python is cool!!!', r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' is', ' cool']\n"
     ]
    }
   ],
   "source": [
    "s = 'Python is cool!!!'\n",
    "print(re.findall(r'\\s\\w+\\b', s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams and Collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Python', 'is'),\n",
       " ('is', 'an'),\n",
       " ('an', 'awesome'),\n",
       " ('awesome', 'language'),\n",
       " ('language', '.')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'Python is an awesome language.'\n",
    "tokens = nltk.word_tokenize(s)\n",
    "list(nltk.bigrams(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_tokens = genesis.words('english-kjv.txt')\n",
    "eng_bigrams = nltk.bigrams(eng_tokens)\n",
    "filtered_bigrams = [(w1, w2) for w1, w2 in eng_bigrams if len(w1) >=5 and len(w2) >= 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('their', 'father'), 19), (('lived', 'after'), 16), (('seven', 'years'), 15)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_bifreq = nltk.FreqDist(filtered_bigrams)\n",
    "eng_bifreq.most_common(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('creature', 7), ('thing', 4)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_tokens = genesis.words('english-kjv.txt')\n",
    "eng_bigrams = nltk.bigrams(eng_tokens)\n",
    "eng_cfd = nltk.ConditionalFreqDist(eng_bigrams)\n",
    "eng_cfd['living'].most_common(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating Most Frequent Next Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(cfd, word, n=5):\n",
    "    n_words = []\n",
    "    for i in range(n):\n",
    "        n_words.append(word)\n",
    "        word = cfd[word].max()\n",
    "    return n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['living', 'creature', 'that', 'he', 'said']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(eng_cfd, 'living')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Python', 'is', 'an', 'awesome'),\n",
       " ('is', 'an', 'awesome', 'language'),\n",
       " ('an', 'awesome', 'language', '.')]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(nltk.ngrams(tokens, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "said unto; pray thee; thou shalt; thou hast; thy seed; years old;\n",
      "spake unto; thou art; LORD God; every living; God hath; begat sons;\n",
      "seven years; shalt thou; little ones; living creature; creeping thing;\n",
      "savoury meat; thirty years; every beast\n"
     ]
    }
   ],
   "source": [
    "gen_text = nltk.Text(eng_tokens)\n",
    "gen_text.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Python', 'is', 'cool'),\n",
       " ('is', 'cool', '!'),\n",
       " ('cool', '!', '!'),\n",
       " ('!', '!', '!')]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(nltk.trigrams(nltk.word_tokenize('Python is cool!!!')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Text: Monty Python and the Holy Grail>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO: Make the below code generic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({('clop', 'clop'): 26})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text6_bigrams = nltk.bigrams(text6)\n",
    "filtered_text6_bigrams = [(w1, w2) for w1, w2 in text6_bigrams if w1 =='clop' and w2 == 'clop']\n",
    "nltk.FreqDist(filtered_text6_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = [(w1, w2) for w1, w2 in text6_bigrams]\n",
    "nltk.FreqDist(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEMMING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'builder'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import PorterStemmer\n",
    "porter = nltk.PorterStemmer()\n",
    "porter.stem('builders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'build'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import LancasterStemmer\n",
    "lancaster = LancasterStemmer()\n",
    "lancaster.stem('builders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19317\n",
      "17231\n"
     ]
    }
   ],
   "source": [
    "print(len(set(text1)))\n",
    "lc_words = [word.lower() for word in text1] \n",
    "print(len(set(lc_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10927"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_stem_words = [porter.stem(word) for word in set(lc_words)]\n",
    "len(set(p_stem_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9036"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_stem_words = [lancaster.stem(word) for word in set(lc_words)]\n",
    "len(set(l_stem_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemma** is a lexical entry in a lexical resource such as word dictionary.\n",
    "\n",
    "You can find multiple Lemma's with the same spelling. These are known as homonyms.\n",
    "\n",
    "For example, consider the two Lemma's listed below, which are homonyms.\n",
    "\n",
    "1. saw [verb] - Past tense of see\n",
    "2. saw [noun] - Cutting instrument\n",
    "\n",
    "nltk comes with **WordNetLemmatizer**. This lemmatizer removes affixes only if the resulting word is found in lexical resource, **Wordnet**.\n",
    "\n",
    "**WordNetLemmatizer** is majorly used to build a vocabulary of words, which are valid Lemmas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15168"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnl = nltk.WordNetLemmatizer()\n",
    "wnl_stem_words = [wnl.lemmatize(word) for word in set(lc_words)]\n",
    "len(set(wnl_stem_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find how many words ending with 'ing' in text6 corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "281"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ing = [word for word in text6 if word.endswith('ing')]\n",
    "len(ing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**POS Tagging**\n",
    "\n",
    "The method of categorizing words into their parts of speech and then labeling them respectively is called POS Tagging.\n",
    "\n",
    "A **POS Tagger** processes a sequence of words and tags a part of speech to each word.\n",
    "\n",
    "**pos_tag** is the simplest tagger available in nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Python', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('an', 'DT'),\n",
       " ('interpreted', 'JJ'),\n",
       " ('high-level', 'NN'),\n",
       " ('programming', 'NN'),\n",
       " ('language', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('general-purpose', 'JJ'),\n",
       " ('programming', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Created', 'VBN'),\n",
       " ('by', 'IN'),\n",
       " ('Guido', 'NNP'),\n",
       " ('van', 'NN'),\n",
       " ('Rossum', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('first', 'RB'),\n",
       " ('released', 'VBN'),\n",
       " ('in', 'IN'),\n",
       " ('1991', 'CD'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = nltk.word_tokenize(text)\n",
    "nltk.pos_tag(words)\n",
    "# The words Python, is and awesome are tagged to Proper Noun (NNP), Present Tense Verb (VB), and adjective (JJ) respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n"
     ]
    }
   ],
   "source": [
    "# Help command - Remove the argument for full details\n",
    "nltk.help.upenn_tagset('JJ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tagging Text**\n",
    "\n",
    "Constructing a list of tagged words from a string is possible.\n",
    "\n",
    "A tagged word or token is represented in a tuple, having the word and the tag.\n",
    "\n",
    "In the input text, each word and tag are separated by /."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Python', 'NN'), ('is', 'VB'), ('awesome', 'JJ'), ('.', '.')]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Python/NN is/VB awesome/JJ ./.'\n",
    "[nltk.tag.str2tuple(word) for word in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL')]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# brown corpus is an example of Tagged Corpora\n",
    "brown_tagged = brown.tagged_words()\n",
    "brown_tagged[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DefaultTagger** assigns a specified tag to every word or token of given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Python', 'NN'), ('is', 'NN'), ('awesome', 'NN'), ('.', 'NN')]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Python is awesome.'\n",
    "words = nltk.word_tokenize(text)\n",
    "default_tagger = nltk.DefaultTagger('NN')\n",
    "default_tagger.tag(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unigram Tagger ####\n",
    "\n",
    "UnigramTagger provides us the flexibility to create our own taggers.\n",
    "\n",
    "Unigram taggers are built based on statistical information. i.e., they tag each word or token to **most likely** tag for that particular word.\n",
    "\n",
    "We can build a unigram tagger through a process known as **training**.\n",
    "\n",
    "Then use the tagger to tag words in a test set and evaluate the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom tagger\n",
    "defined_tags = {'is':'BEZ', 'over':'IN', 'who': 'WPS'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Python', None), ('is', 'BEZ'), ('awesome', None), ('.', None)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_tagger = nltk.UnigramTagger(model=defined_tags)\n",
    "baseline_tagger.tag(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the tagged sentences of **brown** corpus collections, associated with **government** genre.\n",
    "\n",
    "Let's also compute the training set size, i.e., 80%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3032\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2425"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown_tagged_sents = brown.tagged_sents(categories='government')\n",
    "brown_sents = brown.sents(categories='government')\n",
    "print(len(brown_sents))\n",
    "train_size = int(len(brown_sents)*0.8)\n",
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7799495586380832"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sents = brown_tagged_sents[:train_size]\n",
    "test_sents = brown_tagged_sents[train_size:]\n",
    "unigram_tagger = nltk.UnigramTagger(train_sents)\n",
    "unigram_tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'AT'),\n",
       " ('first', 'OD'),\n",
       " ('step', 'NN'),\n",
       " ('is', 'BEZ'),\n",
       " ('a', 'AT'),\n",
       " ('comprehensive', 'JJ'),\n",
       " ('self', None),\n",
       " ('study', 'NN'),\n",
       " ('made', 'VBN'),\n",
       " ('by', 'IN'),\n",
       " ('faculty', None),\n",
       " (',', ','),\n",
       " ('by', 'IN'),\n",
       " ('outside', 'IN'),\n",
       " ('consultants', 'NNS'),\n",
       " (',', ','),\n",
       " ('or', 'CC'),\n",
       " ('by', 'IN'),\n",
       " ('a', 'AT'),\n",
       " ('combination', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'AT'),\n",
       " ('two', 'CD'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict tagging for a sentence taken from the test set\n",
    "unigram_tagger.tag(brown_sents[3000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('fly', 'NN')\n"
     ]
    }
   ],
   "source": [
    "tagged_token = nltk.tag.str2tuple('fly/NN')\n",
    "print(tagged_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which tag occurs maximum in text collections associated with **news** genre of **brown** corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.corpus.reader.util.ConcatenatedCorpusView"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(brown_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NN'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = [tag for (word, tag) in brown.tagged_words(categories='news')]\n",
    "nltk.FreqDist(tags).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Python', 'NNP'), ('is', 'VBZ'), ('awesome', 'JJ')]\n"
     ]
    }
   ],
   "source": [
    "s = 'Python is awesome'\n",
    "print(nltk.pos_tag(nltk.word_tokenize(s)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "\n",
    "Tokenizing text using functions **word_tokenize** and **sent_tokenize**.\n",
    "\n",
    "Computing Frequencies with **FreqDist** and **ConditionalFreqDist**.\n",
    "\n",
    "Generating Bigrams and collocations with **bigrams** and **collocations**.\n",
    "\n",
    "Stemming word affixes using **PorterStemmer** and **LancasterStemmer**.\n",
    "\n",
    "Tagging words to their parts of speech using **pos_tag**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
