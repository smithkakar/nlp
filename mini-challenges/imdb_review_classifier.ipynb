{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge: IMDB Review Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run the Cell to import the packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fill in the Command to load your CSV dataset \"imdb.csv\" with pandas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index                                               text  label\n",
      "0      0  A very, very, very slow-moving, aimless movie ...      0\n",
      "1      1  Not sure who was more lost - the flat characte...      0\n",
      "2      2  Attempting artiness with black & white and cle...      0\n",
      "3      3       Very little music or anything to speak of.        0\n",
      "4      4  The best scene in the movie was when Gerardo i...      1\n"
     ]
    }
   ],
   "source": [
    "imdb = pd.read_csv('../data/imdb.csv')\n",
    "imdb.columns = [\"index\", \"text\", \"label\"]\n",
    "print(imdb.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Analysis**\n",
    "\n",
    "- Get the shape of the dataset and print it.\n",
    "\n",
    "- Get the column names in list and print it.\n",
    "\n",
    "- Group the dataset by **label** and describe the dataset to understand the basic statistics of the dataset.\n",
    "\n",
    "- Print the first three rows of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  (1000, 3)\n",
      "columns:  ['index', 'text', 'label']\n",
      "       index                                                        \n",
      "       count     mean         std  min     25%    50%     75%    max\n",
      "label                                                               \n",
      "0      500.0  466.418  276.272620  0.0  218.75  462.5  700.25  999.0\n",
      "1      500.0  532.582  297.457084  4.0  297.75  569.5  787.25  993.0\n",
      "   index                                               text  label\n",
      "0      0  A very, very, very slow-moving, aimless movie ...      0\n",
      "1      1  Not sure who was more lost - the flat characte...      0\n",
      "2      2  Attempting artiness with black & white and cle...      0\n"
     ]
    }
   ],
   "source": [
    "data_size = imdb.shape\n",
    "print(\"shape: \", data_size)\n",
    "imdb_col_names = list(imdb.columns)\n",
    "print(\"columns: \", imdb_col_names)\n",
    "print(imdb.groupby(['label']).describe())\n",
    "print(imdb.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Target Identification**\n",
    "\n",
    "Execute the below cell to identify the target variables. If `0`, then it is a bad review and if `1` it is a good review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    500\n",
      "1    500\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "imdb_target=imdb['label'] \n",
    "print(imdb_target.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenization**\n",
    "\n",
    "- Convert the text into lower.\n",
    "- Tokenize the text using word_tokenize\n",
    "- Apply the function **split_tokens** for the column **text** in the **imdb** dataset with axis =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('all')\n",
    "def split_tokens(text):\n",
    "    message = text.lower()\n",
    "    word_tokens = word_tokenize(message)\n",
    "    return word_tokens\n",
    "\n",
    "imdb['tokenized_message'] = imdb.apply(lambda row: split_tokens(row['text']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemmatization**\n",
    "\n",
    "- Apply the function **split_into_lemmas** for the column **tokenized_message** with axis=1\n",
    "- Print the 55th row from the column **tokenized_message**.\n",
    "- Print the 55th row from the column **lemmatized_message**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized message: ['the', 'movie', 'showed', 'a', 'lot', 'of', 'florida', 'at', 'it', \"'s\", 'best', ',', 'made', 'it', 'look', 'very', 'appealing', '.']\n",
      "Lemmatized message: ['the', 'movie', 'showed', 'a', 'lot', 'of', 'florida', 'at', 'it', \"'s\", 'best', ',', 'made', 'it', 'look', 'very', 'appealing', '.']\n"
     ]
    }
   ],
   "source": [
    "def split_into_lemmas(text):\n",
    "    lemma = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for word in text:\n",
    "        a = lemmatizer.lemmatize(word)\n",
    "        lemma.append(a)\n",
    "    return lemma\n",
    "\n",
    "imdb['lemmatized_message'] = imdb.apply(lambda row: split_into_lemmas(row['tokenized_message']), axis=1)\n",
    "print('Tokenized message:',  imdb['tokenized_message'][11])\n",
    "print('Lemmatized message:', imdb['lemmatized_message'][11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stop Word Removal**\n",
    "- Set the stop words language as english in the variable **stop_words**\n",
    "- Apply the function **stopword_removal** to the column **lemmatized_message** with axis=1\n",
    "- Print the 55th row from the column **preprocessed_message**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed message: movie showed lot florida 's best , made look appealing .\n"
     ]
    }
   ],
   "source": [
    "def stopword_removal(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_sentence = []\n",
    "    filtered_sentence = ' '.join([word for word in text if word not in stop_words])\n",
    "    return filtered_sentence\n",
    "\n",
    "imdb['preprocessed_message'] = imdb.apply(lambda row: stopword_removal(row['lemmatized_message']), axis=1)\n",
    "print('Preprocessed message:', imdb['preprocessed_message'][11])\n",
    "X = pd.Series(list(imdb['preprocessed_message']))\n",
    "y = pd.Series(list(imdb['label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Term Document Matrix**\n",
    "\n",
    "- Apply CountVectorizer with following parameters\n",
    "  - ngram_range = (1,2)\n",
    "  - min_df = (1/len(Training_label))\n",
    "  - max_df = 0.7\n",
    "- Fit the **tf_vectorizer** with the **X**\n",
    "- Transform the **total_dictionary_tdm** with the **X** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vectorizer = CountVectorizer(ngram_range=(1, 2), min_df=(1/len(y)), max_df=0.7)\n",
    "total_dictionary_tdm = tf_vectorizer.fit(X, y)\n",
    "message_data_tdm = total_dictionary_tdm.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Term Frequency Inverse Document Frequency (TFIDF)**\n",
    "- Apply TfidfVectorizer with following parameters\n",
    "  - ngram_range = (1,2)\n",
    "  - min_df = (1/len(Training_label))\n",
    "  - max_df = 0.7\n",
    "- Fit the **tfidf_vectorizer** with the **X**\n",
    "- Transform the **total_dictionary_tfidf** with the **X** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), min_df=(1/len(y)), max_df=0.7)\n",
    "total_dictionary_tfidf = tfidf_vectorizer.fit(X, y)\n",
    "message_data_tfidf = total_dictionary_tfidf.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train and Test Data**\n",
    "\n",
    "Splitting the data for training and testing (90% train,10% test)\n",
    "\n",
    "- Perform train-test split on `message_data_tdm` and `y` with 90% as train data and 10% as test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, train_label, test_label = train_test_split(message_data_tdm, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Support Vector Machine**\n",
    "\n",
    "- Get the shape of the train-data and print the same.\n",
    "\n",
    "- Get the shape of the test-data and print the same.\n",
    "\n",
    "- Initialize SVM classifier with following parameters\n",
    "    - kernel = linear\n",
    "    - C= 0.025\n",
    "    - random_state=seed\n",
    "\n",
    "- Train the model with train_data and train_label\n",
    "\n",
    "- Now predict the output with test_data\n",
    "\n",
    "- Evaluate the classifier with score from test_data and test_label\n",
    "\n",
    "- Print the predicted score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of train data:  (900, 9051)\n",
      "The shape of test data:  (100, 9051)\n",
      "SVM Classifier :  0.76\n"
     ]
    }
   ],
   "source": [
    "seed = 9\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "train_data_shape = train_data.shape\n",
    "test_data_shape = test_data.shape\n",
    "print(\"The shape of train data: \", train_data_shape)\n",
    "print(\"The shape of test data: \", test_data_shape)\n",
    "classifier = SVC(kernel='linear', C=0.025, random_state=seed)\n",
    "classifier = classifier.fit(train_data, train_label)\n",
    "target = classifier.predict(test_data)\n",
    "score = classifier.score(test_data, test_label)\n",
    "print('SVM Classifier : ', score)\n",
    "\n",
    "# with open('output.txt', 'w') as file:\n",
    "#     file.write(str((imdb['tokenized_message'][55], imdb['lemmatized_message'][55])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stochastic Gradient Descent Classifier**\n",
    "\n",
    "- Perform train-test split on `message_data_tdm` and `y` with this time 80% as train data and 20% as test data.\n",
    "\n",
    "- Get the shape of the train-data and print the same.\n",
    "\n",
    "- Get the shape of the test-data and print the same.\n",
    "\n",
    "- Initialize SVM classifier with following parameters\n",
    "    - loss = modified_huber\n",
    "    - shuffle= True\n",
    "    - random_state=seed\n",
    "\n",
    "- Train the model with train_data and train_label\n",
    "\n",
    "- Now predict the output with test_data\n",
    "\n",
    "- Evaluate the classifier with score from test_data and test_label\n",
    "\n",
    "- Print the predicted score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of train data:  (900, 9051)\n",
      "The shape of test data:  (100, 9051)\n",
      "SGD classifier :  0.79\n"
     ]
    }
   ],
   "source": [
    "seed = 9\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "train_data,test_data, train_label, test_label = train_test_split(message_data_tdm, y, test_size=0.1)\n",
    "train_data_shape = train_data.shape\n",
    "test_data_shape = test_data.shape\n",
    "print(\"The shape of train data: \", train_data_shape)\n",
    "print(\"The shape of test data: \", test_data_shape)\n",
    "classifier =  SGDClassifier(loss='modified_huber', shuffle=True, random_state=seed)\n",
    "classifier = classifier.fit(train_data, train_label)\n",
    "target = classifier.predict(test_data)\n",
    "score = classifier.score(test_data, test_label)\n",
    "print('SGD classifier : ',score)\n",
    "\n",
    "# with open('output1.txt', 'w') as file:\n",
    "#     file.write(str((imdb['preprocessed_message'][55])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
