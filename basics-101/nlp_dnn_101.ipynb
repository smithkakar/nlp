{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole point of training a **skip gram** model is not to predict the context word given the target word but is to learn the weights of the **embedding matrix**.\n",
    "\n",
    "This is true for all other models that are used to compute the **word embeddings**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gensim**\n",
    "\n",
    "Gensim is an open source library for natural language processing.\n",
    "\n",
    "Using minimal lines of code we can generate word vectors for our own corpus.\n",
    "\n",
    "**Word2Vec Parameters**\n",
    "\n",
    "size: (default 100) The number of dimensions of the embedding, e.g., the length of the dense vector to represent each token (word).\n",
    "\n",
    "window: (default 5) The maximum distance between a target word and words around the target word.\n",
    "\n",
    "min_count: (default 5) The minimum count of words to consider when training the model; words with an occurrence less than this count will be ignored.\n",
    "\n",
    "workers: (default 3) The number of threads to use while training.\n",
    "\n",
    "sg: (default 0 or CBOW) The training algorithm, either CBOW (0) or skip gram (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=26, size=10, alpha=0.025)\n",
      "['gensim', 'is', 'billed', 'as', 'a', 'natural', 'language', 'processing', 'package', 'but', 'it', 'practically', 'much', 'more', 'than', 'that', 'It', 'leading', 'and', 'state', 'of', 'the', 'art', 'for', 'texts', 'workingwithwordvectormodels']\n",
      "[-0.03448543 -0.02987814 -0.03394822 -0.0402063   0.01721926  0.00844841\n",
      " -0.01756344  0.01706514 -0.03758939 -0.02017645]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/smithkakar/anaconda3/envs/nlp/lib/python3.7/site-packages/ipykernel_launcher.py:20: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# define training data\n",
    "sentences = [['gensim', 'is', 'billed','as', 'a', 'natural', 'language', 'processing', 'package'],\n",
    "            ['but', 'it', 'is', 'practically', 'much', 'more', 'than' ,'that'],\n",
    "            ['It', 'is', 'a', 'leading', 'and', 'a', 'state', 'of', 'the', 'art', 'package', 'for', 'processing', \n",
    "             'texts', 'working' 'with' 'word' 'vector' 'models']]\n",
    "\n",
    "# train model\n",
    "model = Word2Vec(sentences, min_count=1, size = 10)\n",
    "\n",
    "# summarize the loaded model\n",
    "print(model)\n",
    "\n",
    "# summarize vocabulary\n",
    "words = list(model.wv.vocab)\n",
    "print(words)\n",
    "\n",
    "# access vector for one word\n",
    "print(model['gensim'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing\n",
    "\n",
    "When it comes to text data, we first remove all kinds of stop words, if necessary, and then transform each character or words into one hot encoding.\n",
    "Keras framework has a built-in class called Tokenizer which performs implicit tokenization and indexing of each words in the document.\n",
    "\n",
    "It also eliminates special characters in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "OrderedDict([('not', 1), ('good', 1), ('climax', 1), ('was', 1), ('awesome', 2), ('really', 1), ('liked', 1), ('the', 1), ('movie', 1), ('too', 1), ('lengthy', 1)])\n",
      "{'awesome': 1, 'not': 2, 'good': 3, 'climax': 4, 'was': 5, 'really': 6, 'liked': 7, 'the': 8, 'movie': 9, 'too': 10, 'lengthy': 11}\n",
      "defaultdict(<class 'int'>, {'not': 1, 'good': 1, 'awesome': 2, 'climax': 1, 'was': 1, 'movie': 1, 'the': 1, 'really': 1, 'liked': 1, 'too': 1, 'lengthy': 1})\n"
     ]
    }
   ],
   "source": [
    "### collection of text (or corpus)\n",
    "docs = [\"not good\",  \"climax was awesome !\",  \"really liked the movie\",  \"too lengthy\", \"awesome!\"]\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "t = Tokenizer()\n",
    "\n",
    "### Perform transformation\n",
    "t.fit_on_texts(docs)\n",
    "\n",
    "###Output the number of documents in the corpus\n",
    "print(t.document_count)\n",
    "\n",
    "###Output the number of occurrence of each word across the document\n",
    "print(t.word_counts)\n",
    "\n",
    "###Output the dictionary having word as key and their unique index as values\n",
    "print(t.word_index)\n",
    "\n",
    "###Output the dictionary having word as key and number of documents it has appeared as values\n",
    "print(t.word_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lookup Table\n",
    "\n",
    "Now each word in text data is replaced by their respective index.\n",
    "\n",
    "To train the LSTM model, we will not directly input the word index to the LSTM network.\n",
    "\n",
    "We first initialize the lookup table of shape (vocab_size, vector_length).\n",
    "\n",
    "We do this in KerasEmbedding class as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding  \n",
    "\n",
    "embedding_layer = Embedding(vocab_size,  vector_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transform Data**\n",
    "\n",
    "Once you have the unique index for each word in the corpus, the corpus has to be represented as an array of an index in place of words as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id = { \"the\": 0, \"awesome\": 1, \"movie\": 2, \"good\":3, \"was\":4 }\n",
    " \n",
    "data = [[\"the movie was awesome\"]]\n",
    "transformed_data = [[0, 2, 4, 1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequence Padding\n",
    "\n",
    "    - The length of the movie review is not always determined, it can be too short or too long.\n",
    "    - The model may take a very long time to train if the text data is too long.\n",
    "    - For all the reviews we may consider only first few words say 500.\n",
    "    - If the text is less than 500words we zeros, in the beginning, to make up the length to 500 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 4, 1]], dtype=int32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing import sequence \n",
    "max_review_length = 100\n",
    "sequence.pad_sequences(transformed_data,  maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
