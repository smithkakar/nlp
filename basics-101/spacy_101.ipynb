{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**spaCy** is a free and open source library for doing advanced Natural Language Processing (NLP).\n",
    "\n",
    "- It supports more than 20 languages.\n",
    "- Provides ready-made statistical models for many languages.\n",
    "- Interoperability with popular machine learning libraries like PyTorch, TensorFlow, and Scikit-learn.\n",
    "- Cross-platform.\n",
    "- Offers the fastest syntactic parser in the world.\n",
    "\n",
    "**Features:**\n",
    "\n",
    "- spaCy is written using Python and Cython.\n",
    "- Unlike NLTK, which is more teaching and research-oriented, spaCy is more performance oriented and is hence more common in production environments.\n",
    "- Built especially for a production environment.\n",
    "- Helps to process and analyze large collections of text data.\n",
    "\n",
    "The features offered by spaCy include:\n",
    "\n",
    " - Tokenization\n",
    " - Part-of-speech (POS) tagging\n",
    " - Lemmatization\n",
    " - Sentence Boundary Detection (SBD)\n",
    " - Named Entity Recognition (NER)\n",
    " - Similarity\n",
    "    \n",
    "Note: Stemming and Lemmatization are Normalization techniques in NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! python -m spacy download en \n",
    "#! python -m spacy download fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpaCy is an amazing tool like NLTK"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docx = nlp(u'SpaCy is an amazing tool like NLTK')\n",
    "docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SpaCy', 'is', 'an', 'amazing', 'tool', 'like', 'NLTK']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [token.text for token in docx]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenization**\n",
    "\n",
    "In spaCy, tokens are represented by objects of the Token class. spaCy can tokenize the data into individual tokens based on the rules of the language so that they can be analyzed more efficiently.\n",
    "\n",
    "Though the task of tokenization may be as simple as splitting the sentences based on the appearance of spaces at first, it can be a much more complicated task, which is intricately linked with the language in question.\n",
    "\n",
    "For example, in \"What's your name?\", the tokens are \"What\" and \"'s\" and not just \"What's\".\n",
    "\n",
    "spaCy stores the strings associated with the tokens in **hashed form** to save space.\n",
    "\n",
    "**Sentence Boundary Detection**\n",
    "\n",
    "The input text data often needs to be split into sentences for NLP. The process of identifying parts of the input data that can be classified as sentences is known as sentence boundary detection.\n",
    "\n",
    "This task can be complicated due to the presence of dots as part of names, web addresses, etc.\n",
    "\n",
    "For example,\n",
    "\n",
    "The author, H. G. Wells, wrote a lot of other books.\n",
    "\n",
    "A naive detection algorithm may be fooled into thinking that each dot ends a sentence, which is not the case.\n",
    "\n",
    "spaCy intelligently performs sentence boundary detection considering such details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mycustom_boundary(docx):\n",
    "    for token in docx[:-1]:\n",
    "        if token.text == '---':\n",
    "            docx[token.i+1].is_sent_start = True\n",
    "            \n",
    "    return docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the rule before parsing\n",
    "nlp.add_pipe(mycustom_boundary, before='parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysentence = nlp(u\"Hello world---NLP---Spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world---\n",
      "NLP---\n",
      "Spacy\n"
     ]
    }
   ],
   "source": [
    "for sentence in mysentence.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Use **from spacy.pipeline import SentenceSegmenter** for Custom Sentence Boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition\n",
    "\n",
    "A **named entity** is a real-world object that is given a name. For example, a person who is given a name like 'John' or a book with the title 'One Hundred Years of Solitude'.\n",
    "\n",
    "Named entity recognition (NER) is the process of identifying the parts of the input text and classifying them into a set of pre-defined categories or named entities.\n",
    "\n",
    "spaCy is capable of performing NER with the assistance of statistical models.\n",
    "\n",
    "spaCy has some pre-trained models, but they need to be fine-tuned to fit our needs more specifically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Similarity**\n",
    "\n",
    "spaCy can compare two entities and predict how similar they are. This ability is highly useful in identifying duplicate entries and in finding recommendations.\n",
    "\n",
    "Every Token object has a similarity() method which gives a number. Higher this number, higher the similarity.\n",
    "\n",
    "This means, a token when compared with itself would give the maximum similarity score (which may not always be 1 due to vector math and floating point imprecisions).\n",
    "\n",
    "See the table below to see an example with three words:\n",
    "\n",
    "                dog\t cat\tbanana\n",
    "        dog    1.00\t0.80\t0.24\n",
    "        cat\t0.80\t1.00\t0.28\n",
    "     banana\t0.24\t0.28\t1.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6759108589205962"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1 = nlp(\"wolf\")\n",
    "doc2 = nlp(\"dog\")\n",
    "doc1.similarity(doc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpaCy Architecture\n",
    "\n",
    "#### Processing Pipeline\n",
    "\n",
    "When the model of spaCy is used, spaCy performs tokenization on the input text to give an object of the **Doc** class. This Doc object is further processed in a procedure involving a set of processes known as the processing pipeline.\n",
    "\n",
    "spaCy has some default models that are optimized for performance. However, custom models can also be developed.\n",
    "\n",
    "The default pipeline consists of:\n",
    "\n",
    "- a tagger for identifying the POS tags,\n",
    "- a parser\n",
    "- an entity recognizer\n",
    "    \n",
    "Each of these parts takes the Doc object and returns a processed version of it, which is another Doc object and passes it on to the next component in the pipeline.\n",
    "\n",
    "The primary data structures in spaCy are:\n",
    "\n",
    "- Doc\n",
    "- Vocab\n",
    "    \n",
    "Doc object has the collection of tokens from the input text data.\n",
    "\n",
    "Vocab object provides a set of look-up tables, which ensures that common information is available across documents.\n",
    "\n",
    "Strings and word vectors are stored in a centralized manner and there is only one source of this data, which ensures integrity.\n",
    "\n",
    "A Doc object is a collection of Token objects.\n",
    "\n",
    "**Span** and **Token** objects are effectively mere views of the parts of the Doc object. The real data is owned by the Doc.\n",
    "\n",
    "Doc object has many **metadata** including **annotations** and other **linguistic** information.\n",
    "\n",
    "Slicing a Doc object produces Span objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[natural, language, processing, .]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# doc is a Doc object and doc_slice is a Span object.\n",
    "doc  = [x for x in nlp('Spacy is used for natural language processing.')]\n",
    "doc_slice = doc[4:]\n",
    "doc_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "quick brown"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"The quick brown fox jumps over the lazy dog\")\n",
    "span = doc[1:3]\n",
    "span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Spacy, is, used, for, doing, natural, language, processing, .]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokens\n",
    "# Note that even the full stop denoting the end of the sentence will be a Token.\n",
    "doc = nlp('Spacy is used for doing natural language processing.')\n",
    "l = [token for token in doc]\n",
    "l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### String Store\n",
    "\n",
    "spaCy stores all the strings in the data that it is handling in a centralized manner in a location called the **String Store**.\n",
    "\n",
    "spaCy handles the strings in terms of its **hashes** as it saves space.\n",
    "\n",
    "When the string version of the 64-bit hash is needed, SpaCy consults the string store to obtain it.\n",
    "\n",
    "The 'single-source' way of storing the string ensures integrity and consistency.\n",
    "\n",
    "If we get a hash version of a string using the attribute of an object, we can get the string version by appending an underscore (_) to the attribute's name.\n",
    "\n",
    "For example, we can get the hash form of the POS tag of a Token object via its **pos** attribute. We can also get the string version using **pos_**.\n",
    "\n",
    "#### Vocab and Lexeme\n",
    "\n",
    "An object of the Vocab class in spaCy stores the words or vocabulary along with other data shared across a particular language.\n",
    "\n",
    "Each entry in an object of the Vocab class is known as a **Lexeme**. Unlike a Token, a Lexeme has **no contextual information** like POS tag. It is just a word type.\n",
    "\n",
    "#### POS Tagging\n",
    "\n",
    "spaCy can use statistical models to analyze the input text data and predict the tag or label of the constituent words.\n",
    "\n",
    "The input text would have been first made into a Doc object.\n",
    "\n",
    "This process is known as Part of Speech Tagging or POS tagging.\n",
    "\n",
    "The statistical model used is trained by showing numerous examples. For instance, the word that comes after a \"the\" in a sentence is probably a noun (NN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(Spacy, 'PROPN'), (is, 'VERB'), (used, 'VERB'), (for, 'ADP'), (doing, 'VERB'), (natural, 'ADJ'), (language, 'NOUN'), (processing, 'NOUN'), (., 'PUNCT')]\n"
     ]
    }
   ],
   "source": [
    "d = nlp('Spacy is used for doing natural language processing.')\n",
    "# POS tags predicted by the English model.\n",
    "print([(token, token.pos_) for token in d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'adposition'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Help tool for POS tags\n",
    "spacy.explain('ADP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sally PROPN NNP advmod\n",
      "likes VERB VBZ ROOT\n",
      "Sam PROPN NNP dobj\n"
     ]
    }
   ],
   "source": [
    "ex = nlp('Sally likes Sam')\n",
    "for word in ex:\n",
    "    print(word.text, word.pos_, word.tag_, word.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'adverbial modifier'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('advmod')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize Dependency using displaCy\n",
    "\n",
    "displaCy is used for visualising the syntactic dependencies and POS tags.\n",
    "\n",
    "displaCy ENT is used to visualize the named entities. It highlights the named entities and their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "# Below code currently not working with the installed version...\n",
    "#displacy.render(ex, style='dep', jupyter=True)\n",
    "# The below will create a simple web server with the visualization which we can view using a web browser.\n",
    "#displacy.serve(ex, style='dep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5\">A demo of \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    displaCy\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ".</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc1 = nlp('A demo of displaCy.')\n",
    "#displacy.serve(doc, style='dep')\n",
    "displacy.render(doc1, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Leo Tolstoy\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " wrote '\n",
       "<mark class=\"entity\" style=\"background: #f0d0ff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    War and Peace'\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">WORK_OF_ART</span>\n",
       "</mark>\n",
       ".</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc2 = nlp(\"Leo Tolstoy wrote 'War and Peace'.\")\n",
    "#displacy.serve(doc, style='ent')\n",
    "displacy.render(doc2, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization as Raw HTML\n",
    "\n",
    "If we do not want to set up a web server just for the sake of seeing the visualization, displaCy can give us the HTML code that can be used to display it.\n",
    "\n",
    "The displacy.render() method, with its **page** argument **True**, can be used for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# html = displacy.render([doc1, doc2], style='dep', page=True)\n",
    "# Now the variable html will have the HTML code for generating the visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Generations, 'NOUN'),\n",
       " (to, 'PART'),\n",
       " (come, 'VERB'),\n",
       " (may, 'VERB'),\n",
       " (not, 'ADV'),\n",
       " (believe, 'VERB'),\n",
       " (that, 'ADP'),\n",
       " (such, 'ADJ'),\n",
       " (a, 'DET'),\n",
       " (man, 'NOUN'),\n",
       " (lived, 'VERB'),\n",
       " (., 'PUNCT')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = nlp(\"Generations to come may not believe that such a man lived.\")\n",
    "[(word, word.pos_) for word in temp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Spacy', 'GPE')]\n"
     ]
    }
   ],
   "source": [
    "doc  = nlp('Spacy is used for natural language processing.')\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Leo Tolstoy', 'PERSON'), ('Russian', 'NORP'), (\"War and Peace'\", 'WORK_OF_ART')]\n"
     ]
    }
   ],
   "source": [
    "test = nlp(\"Leo Tolstoy, the great Russian writer, is well known for his magnum opus 'War and Peace'.\")\n",
    "print([(ent.text, ent.label_) for ent in test.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nationalities or religious or political groups'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('NORP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop words\n",
    "\n",
    "One of the first steps done in **early information retrieval systems** which were usually based on keyword searching was the removal of stop words.\n",
    "\n",
    "Stop words are words which most frequently appear in the sentences of a language like conjunctions (\"and\", \"or\", \"but\", etc).\n",
    "\n",
    "The method of removing stop words from input text before it is analyzed can still be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(nlp.vocab['the'].is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# Custom setting of the stop-words\n",
    "nlp.vocab['the'].is_stop = False\n",
    "print(nlp.vocab['the'].is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chunking\n",
    "\n",
    "The process of dividing sentences into segments that do not overlap is known as **text chunking**.\n",
    "\n",
    "Specific extraction of noun phrases in known as **Noun phrase chunking** or **NP chunking**. Noun phrases are important as they are often the keywords and are highly useful in information retrieval systems.\n",
    "\n",
    "Examples of chunks include \"the azure blue sky\", \"world's largest river\", etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Spacy, natural language processing]\n",
      "['Spacy', 'natural language processing']\n",
      "['Spacy', 'processing']\n",
      "[('Spacy', 'used'), ('processing', 'for')]\n"
     ]
    }
   ],
   "source": [
    "doc  = nlp('Spacy is used for natural language processing.')\n",
    "print([x for x in doc.noun_chunks])\n",
    "print([x.text for x in doc.noun_chunks])\n",
    "print([x.root.text for x in doc.noun_chunks])\n",
    "# Tuple root.head.text - connecting text\n",
    "print([(x.root.text, x.root.head.text) for x in doc.noun_chunks])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parser\n",
    "\n",
    "Spacy's syntactic dependency parser is one of the best in the world and is one of its main features that make spaCy stand out. This parser also performs **sentence boundary detection**.\n",
    "\n",
    "We can check if Doc object is parsed using its **is_parsed** boolean attribute which will be True if parsing has been performed.\n",
    "\n",
    "#### Dependency Parsing\n",
    "\n",
    "Spacy constructs a parse tree to find out the **dependencies** between the words in the input text. This dependency tree is useful for **text chunking**.\n",
    "\n",
    "Every token in the parse tree other than the root will have only one parent. The parent of a token in the parse tree is known as its head in spacy terminology. Also, the child tokens of a token, if any, in the parse tree are called its children.\n",
    "\n",
    "The syntactic relation between the head and the child is known as **dep**. The dep of a Token can be obtained from its **dep** attribute.\n",
    "\n",
    "**Subtree of Parse Tree**\n",
    "\n",
    "We can get the whole phrase associated with a Token by using its subtree attribute which will give a generator giving out children of a subtree of the parse tree with that token as root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spacy nsubjpass used []\n",
      "is auxpass used []\n",
      "used ROOT used [Spacy, is, for, .]\n",
      "for prep used [processing]\n",
      "natural amod language []\n",
      "language compound processing [natural]\n",
      "processing pobj for [language]\n",
      ". punct used []\n"
     ]
    }
   ],
   "source": [
    "# Dependency Parsing\n",
    "# children attribute of a Token would give a generator for its children in the parse tree.\n",
    "# text - the text of the token\n",
    "# dep_ - dependency relation as a string\n",
    "# Note: dep would give the hash as Spacy uses hashes to save space.\n",
    "# head.text - parent in a dependency graph as a string\n",
    "# children - the list of children for the token in the dependency graph\n",
    "doc  = nlp('Spacy is used for natural language processing.')\n",
    "for t in doc:\n",
    "  print(t.text, t.dep_, t.head.text, [child for child in t.children])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'auxiliary (passive)'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('auxpass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing\n",
      "[natural, language, processing]\n"
     ]
    }
   ],
   "source": [
    "doc  = nlp('Spacy is used for natural language processing.')\n",
    "print(doc[-2])\n",
    "print([x for x in doc[-2].subtree])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Navigating the Parse Tree**\n",
    "\n",
    "- **left_edge** and **right_edge** attributes of a token will give the left-most (i.e., the first) and the right-most (i.e., the last) tokens in the subtree respectively.\n",
    "- **lefts** and **rights** attributes of a Token returns a generator generating the tokens that appear on its left and right subtrees respectively in the syntactic parse tree.\n",
    "- Similarly **n_lefts** and **n_rights** attributes give the number of tokens in its left and right subtrees respectively.\n",
    "- **ancestors** attribute can be used to iterate over the ancestors of a token.\n",
    "- **is_ancestor()** method can be used to check whether a token is an ancestor of another token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n"
     ]
    }
   ],
   "source": [
    "# Lemma Example\n",
    "ra = nlp(u'ran')\n",
    "for token in ra:\n",
    "    print(token.lemma_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
